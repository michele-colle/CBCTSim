per lanciare e creare il cluster:

gcloud init
gcloud auth login
gcloud cloud-shell ssh --authorize-session
gcloud iam service-accounts enable     --project=cbctsim     185407564243-compute@developer.gserviceaccount.com
gcloud projects add-iam-policy-binding cbctsim     --member=serviceAccount:185407564243-compute@developer.gserviceaccount.com     --role=roles/editor

per creare il cluster:
sudo git clone https://github.com/michele-colle/CBCTSim.git CBCTSim
./gcluster create CBCTSim/HPCScripts/hpc-slurm.yaml     -l ERROR --vars project_id=cbctsim
./gcluster deploy hpc-slurm

nota: per distruggere il cluster (anche direttamente dalla shell di google cloud)
./gcluster destroy hpc-slurm --auto-approve

poi va lanciato l'ssh server dall'istanza VM con scritto slurm-login qualcosa

da la dentro clono il repos
sudo git clone https://github.com/michele-colle/CBCTSim.git CBCTSim
e posso poi lanciare il setup (installazione dell'ambiente spack)

cd CBCTSim/HPCScripts/
chmod +x setup_software.sh
sudo ./setup_software.sh

...da capire come e se si puo ri accedere alle vm sospese..

BUILD DEL SOFTWARE
credo sia da fare nella home direcotry, ho provato a buildare dentro apps ma non ha funzionato..
spack env activate g4_cbct_env
export CC=$(spack location -i llvm@19.1.7)/bin/clang
export CXX=$(spack location -i llvm@19.1.7)/bin/clang++
cmake --preset cpu-release
cmake --build --preset cpu-release -j $(nproc)
python3 generate_macros.py

LANCIO DI UNO SCRIPT
cd HPCScripts/
sbatch test_debug.slurm

ENTRARE IN UNA PARTIZIONE
srun --partition=debug --ntasks=1 --time=00:10:00 --pty bash
